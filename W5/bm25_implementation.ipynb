{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YCNjo-6S8Mog",
    "outputId": "364f4db3-f630-4f26-f3ec-1d4a5d017906"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Dr.\n",
      "Document: Middlemarch.txt, Score: 0.4705\n",
      "Document: Ulysses.txt, Score: 0.3967\n",
      "Document: The Iliad.txt, Score: 0.0649\n",
      "Document: Frankenstein: Or, The Modern Prometheus.txt, Score: 0.0646\n",
      "Document: Romeo and Juliet.txt, Score: 0.0513\n",
      "Document: The Prince.txt, Score: 0.0395\n",
      "Document: Second Treatise of Government.txt, Score: 0.0371\n",
      "Document: The Adventures of Tom Sawyer.txt, Score: 0.0319\n",
      "Document: Grimms' Fairy Tales.txt, Score: 0.0261\n",
      "Document: The Count of Monte Cristo.txt, Score: 0.0232\n",
      "\n",
      "Query: Teacher\n",
      "Document: The Adventures of Tom Sawyer.txt, Score: 0.0957\n",
      "Document: The Prince.txt, Score: 0.0593\n",
      "Document: Frankenstein: Or, The Modern Prometheus.txt, Score: 0.0484\n",
      "Document: Romeo and Juliet.txt, Score: 0.0384\n",
      "Document: The Iliad.txt, Score: 0.0365\n",
      "Document: Middlemarch.txt, Score: 0.0321\n",
      "Document: Ulysses.txt, Score: 0.0279\n",
      "Document: Second Treatise of Government.txt, Score: 0.0279\n",
      "Document: The Count of Monte Cristo.txt, Score: 0.0232\n",
      "Document: Grimms' Fairy Tales.txt, Score: 0.0196\n",
      "\n",
      "Query: Hot\n",
      "Document: Ulysses.txt, Score: 0.3868\n",
      "Document: The Adventures of Tom Sawyer.txt, Score: 0.2489\n",
      "Document: Romeo and Juliet.txt, Score: 0.2460\n",
      "Document: Grimms' Fairy Tales.txt, Score: 0.0941\n",
      "Document: Middlemarch.txt, Score: 0.0898\n",
      "Document: The Iliad.txt, Score: 0.0487\n",
      "Document: The Prince.txt, Score: 0.0474\n",
      "Document: The Count of Monte Cristo.txt, Score: 0.0417\n",
      "Document: Frankenstein: Or, The Modern Prometheus.txt, Score: 0.0387\n",
      "Document: Second Treatise of Government.txt, Score: 0.0223\n",
      "\n",
      "Query: Summer\n",
      "Document: Ulysses.txt, Score: 0.2752\n",
      "Document: The Adventures of Tom Sawyer.txt, Score: 0.2298\n",
      "Document: Frankenstein: Or, The Modern Prometheus.txt, Score: 0.2130\n",
      "Document: Romeo and Juliet.txt, Score: 0.1230\n",
      "Document: Grimms' Fairy Tales.txt, Score: 0.1098\n",
      "Document: The Count of Monte Cristo.txt, Score: 0.0788\n",
      "Document: Middlemarch.txt, Score: 0.0770\n",
      "Document: The Iliad.txt, Score: 0.0682\n",
      "Document: Second Treatise of Government.txt, Score: 0.0446\n",
      "Document: The Prince.txt, Score: 0.0237\n",
      "\n",
      "Query: Cold\n",
      "Document: Frankenstein: Or, The Modern Prometheus.txt, Score: 0.6391\n",
      "Document: Ulysses.txt, Score: 0.4760\n",
      "Document: The Iliad.txt, Score: 0.3410\n",
      "Document: The Count of Monte Cristo.txt, Score: 0.3336\n",
      "Document: The Adventures of Tom Sawyer.txt, Score: 0.2872\n",
      "Document: Grimms' Fairy Tales.txt, Score: 0.2510\n",
      "Document: Middlemarch.txt, Score: 0.2502\n",
      "Document: Romeo and Juliet.txt, Score: 0.2460\n",
      "Document: Second Treatise of Government.txt, Score: 0.0446\n",
      "Document: The Prince.txt, Score: 0.0237\n",
      "\n",
      "Query: Winter\n",
      "Document: Frankenstein: Or, The Modern Prometheus.txt, Score: 0.2711\n",
      "Document: Grimms' Fairy Tales.txt, Score: 0.1412\n",
      "Document: The Iliad.txt, Score: 0.0779\n",
      "Document: Romeo and Juliet.txt, Score: 0.0615\n",
      "Document: Ulysses.txt, Score: 0.0595\n",
      "Document: The Prince.txt, Score: 0.0474\n",
      "Document: Middlemarch.txt, Score: 0.0449\n",
      "Document: Second Treatise of Government.txt, Score: 0.0446\n",
      "Document: The Count of Monte Cristo.txt, Score: 0.0371\n",
      "Document: The Adventures of Tom Sawyer.txt, Score: 0.0191\n",
      "\n",
      "Query: Clothes\n",
      "Document: Grimms' Fairy Tales.txt, Score: 0.4863\n",
      "Document: The Adventures of Tom Sawyer.txt, Score: 0.4021\n",
      "Document: Ulysses.txt, Score: 0.2306\n",
      "Document: Middlemarch.txt, Score: 0.1668\n",
      "Document: The Count of Monte Cristo.txt, Score: 0.1205\n",
      "Document: Frankenstein: Or, The Modern Prometheus.txt, Score: 0.1162\n",
      "Document: Romeo and Juliet.txt, Score: 0.0615\n",
      "Document: The Prince.txt, Score: 0.0474\n",
      "Document: Second Treatise of Government.txt, Score: 0.0446\n",
      "Document: The Iliad.txt, Score: 0.0097\n",
      "\n",
      "Query: live\n",
      "Document: Grimms' Fairy Tales.txt, Score: 0.5704\n",
      "Document: Romeo and Juliet.txt, Score: 0.5591\n",
      "Document: The Count of Monte Cristo.txt, Score: 0.5266\n",
      "Document: Frankenstein: Or, The Modern Prometheus.txt, Score: 0.4930\n",
      "Document: Middlemarch.txt, Score: 0.4432\n",
      "Document: The Prince.txt, Score: 0.4097\n",
      "Document: The Iliad.txt, Score: 0.3719\n",
      "Document: Ulysses.txt, Score: 0.3719\n",
      "Document: Second Treatise of Government.txt, Score: 0.2836\n",
      "Document: The Adventures of Tom Sawyer.txt, Score: 0.2089\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from math import log\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess(text):\n",
    "    return re.findall(r'\\b\\w+\\b', text.lower())\n",
    "\n",
    "# Load documents\n",
    "def load_documents(folder_path):\n",
    "    docs = {}\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith('.txt'):\n",
    "            with open(os.path.join(folder_path, filename), 'r') as file:\n",
    "                docs[filename] = preprocess(file.read())\n",
    "    return docs\n",
    "\n",
    "# Load queries\n",
    "def load_queries(query_file_path):\n",
    "    with open(query_file_path, 'r') as file:\n",
    "        return [line.strip() for line in file.readlines()]\n",
    "\n",
    "# Compute term frequencies and document frequencies\n",
    "def compute_statistics(docs):\n",
    "    doc_count = len(docs)\n",
    "    term_doc_freq = defaultdict(int)\n",
    "    term_freq = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "    for doc_id, words in docs.items():\n",
    "        word_set = set(words)\n",
    "        for word in words:\n",
    "            term_freq[doc_id][word] += 1\n",
    "        for word in word_set:\n",
    "            term_doc_freq[word] += 1\n",
    "\n",
    "    return term_freq, term_doc_freq, doc_count\n",
    "\n",
    "# Compute relevance probabilities using BIM\n",
    "def compute_relevance_prob(query, term_freq, term_doc_freq, doc_count):\n",
    "    scores = {}\n",
    "    for doc_id in term_freq:\n",
    "        score = 0.5\n",
    "        for term in query:\n",
    "            tf = term_freq[doc_id].get(term, 0)\n",
    "            df = term_doc_freq.get(term, 0)\n",
    "            p_term_given_relevant = (tf + 1) / (sum(term_freq[doc_id].values()) + len(term_doc_freq))\n",
    "            p_term_given_not_relevant = (df + 1) / (doc_count - df + len(term_doc_freq))\n",
    "            score *= (p_term_given_relevant / p_term_given_not_relevant)\n",
    "        scores[doc_id] = score\n",
    "    return scores\n",
    "\n",
    "# Main retrieval function\n",
    "def retrieve_documents(folder_path, query_file_path):\n",
    "    docs = load_documents(folder_path)\n",
    "    queries = load_queries(query_file_path)\n",
    "\n",
    "    term_freq, term_doc_freq, doc_count = compute_statistics(docs)\n",
    "\n",
    "    for query in queries:\n",
    "        query_terms = preprocess(query)\n",
    "        scores = compute_relevance_prob(query_terms, term_freq, term_doc_freq, doc_count)\n",
    "        ranked_docs = sorted(scores.items(), key=lambda item: item[1], reverse=True)\n",
    "        print(f\"Query: {query}\")\n",
    "        for doc_id, score in ranked_docs:\n",
    "            print(f\"Document: {doc_id}, Score: {score:.4f}\")\n",
    "        print()\n",
    "\n",
    "# Example usage\n",
    "# folder_path = './Trump Speechs/'\n",
    "# query_file_path = './queries1.txt'\n",
    "\n",
    "folder_path = '../../Final Project/Dataset'\n",
    "query_file_path = '../../Final Project/queries.txt'\n",
    "retrieve_documents(folder_path, query_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kQ0RPexSL23c",
    "outputId": "3ea70c63-cfe3-4daf-e267-5b4dabbaac4b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relevance scores saved to query_relevance_score.txt\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Function to assign random relevance scores\n",
    "def assign_random_relevance(queries, documents, relevance_scale=(0, 1)):\n",
    "    relevance_scores = {}\n",
    "\n",
    "    for query in queries:\n",
    "        relevance_scores[query] = {}  # Use the query string directly, no tuple or list\n",
    "        for doc in documents:\n",
    "            # Assign a random relevance score between relevance_scale (0 and 1 by default)\n",
    "            relevance_scores[query][doc] = random.randint(relevance_scale[0], relevance_scale[1])\n",
    "\n",
    "    return relevance_scores\n",
    "\n",
    "# Function to save relevance scores to a file\n",
    "def save_relevance_scores_to_file(relevance_scores, output_file):\n",
    "    with open(output_file, 'w') as f:\n",
    "        for query, doc_scores in relevance_scores.items():\n",
    "            for doc, score in doc_scores.items():\n",
    "                f.write(f\"{query},{doc},{score}\\n\")  # Write in the format query,document,score\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "folder_path = '../../Final Project/Dataset'\n",
    "query_file_path = '../../Final Project/queries.txt'\n",
    "# Load documents and queries\n",
    "documents = load_documents(folder_path)  # This returns a dictionary of doc_id -> content\n",
    "queries = load_queries(query_file_path)  # This returns a list of queries\n",
    "\n",
    "# Randomly assign relevance scores (0 for irrelevant, 1 for relevant)\n",
    "random_relevance_scores = assign_random_relevance(queries, documents.keys())\n",
    "\n",
    "# Save the relevance scores to query_relevance_score.txt\n",
    "output_file = 'query_relevance_score.txt'\n",
    "save_relevance_scores_to_file(random_relevance_scores, output_file)\n",
    "\n",
    "print(f\"Relevance scores saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "FMJnXjAeRSsu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💡 QUERY: dr\n",
      "==================================================\n",
      "📄 Middlemarch.txt                | 🏆 Score: 1.8931\n",
      "📄 Ulysses.txt                    | 🏆 Score: 1.8588\n",
      "📄 The Iliad.txt                  | 🏆 Score: 1.2455\n",
      "📄 The Count of Monte Cristo.txt  | 🏆 Score: 0.7628\n",
      "📄 Grimms' Fairy Tales.txt        | 🏆 Score: 0.0000\n",
      "📄 Romeo and Juliet.txt           | 🏆 Score: 0.0000\n",
      "📄 Second Treatise of Government.txt | 🏆 Score: 0.0000\n",
      "📄 The Adventures of Tom Sawyer.txt | 🏆 Score: 0.0000\n",
      "📄 The Prince.txt                 | 🏆 Score: 0.0000\n",
      "--------------------------------------------------\n",
      "\n",
      "💡 QUERY: teacher\n",
      "==================================================\n",
      "📄 The Adventures of Tom Sawyer.txt | 🏆 Score: 0.6494\n",
      "📄 Middlemarch.txt                | 🏆 Score: 0.4535\n",
      "📄 The Prince.txt                 | 🏆 Score: 0.4263\n",
      "📄 Second Treatise of Government.txt | 🏆 Score: 0.4188\n",
      "📄 The Iliad.txt                  | 🏆 Score: 0.3777\n",
      "📄 The Count of Monte Cristo.txt  | 🏆 Score: 0.3461\n",
      "📄 Ulysses.txt                    | 🏆 Score: 0.3351\n",
      "📄 Grimms' Fairy Tales.txt        | 🏆 Score: 0.0000\n",
      "📄 Romeo and Juliet.txt           | 🏆 Score: 0.0000\n",
      "--------------------------------------------------\n",
      "\n",
      "💡 QUERY: hot\n",
      "==================================================\n",
      "📄 Ulysses.txt                    | 🏆 Score: 0.3888\n",
      "📄 The Adventures of Tom Sawyer.txt | 🏆 Score: 0.3768\n",
      "📄 Romeo and Juliet.txt           | 🏆 Score: 0.3751\n",
      "📄 Middlemarch.txt                | 🏆 Score: 0.3399\n",
      "📄 Grimms' Fairy Tales.txt        | 🏆 Score: 0.3268\n",
      "📄 The Iliad.txt                  | 🏆 Score: 0.2798\n",
      "📄 The Count of Monte Cristo.txt  | 🏆 Score: 0.2640\n",
      "📄 The Prince.txt                 | 🏆 Score: 0.2408\n",
      "📄 Second Treatise of Government.txt | 🏆 Score: 0.0000\n",
      "--------------------------------------------------\n",
      "\n",
      "💡 QUERY: summer\n",
      "==================================================\n",
      "📄 Ulysses.txt                    | 🏆 Score: 0.3820\n",
      "📄 The Adventures of Tom Sawyer.txt | 🏆 Score: 0.3768\n",
      "📄 Romeo and Juliet.txt           | 🏆 Score: 0.3546\n",
      "📄 The Count of Monte Cristo.txt  | 🏆 Score: 0.3380\n",
      "📄 Middlemarch.txt                | 🏆 Score: 0.3349\n",
      "📄 Grimms' Fairy Tales.txt        | 🏆 Score: 0.3268\n",
      "📄 The Iliad.txt                  | 🏆 Score: 0.3122\n",
      "📄 Second Treatise of Government.txt | 🏆 Score: 0.2366\n",
      "📄 The Prince.txt                 | 🏆 Score: 0.0000\n",
      "--------------------------------------------------\n",
      "\n",
      "💡 QUERY: cold\n",
      "==================================================\n",
      "📄 Ulysses.txt                    | 🏆 Score: 0.3922\n",
      "📄 The Count of Monte Cristo.txt  | 🏆 Score: 0.3893\n",
      "📄 The Iliad.txt                  | 🏆 Score: 0.3863\n",
      "📄 Middlemarch.txt                | 🏆 Score: 0.3858\n",
      "📄 The Adventures of Tom Sawyer.txt | 🏆 Score: 0.3828\n",
      "📄 Grimms' Fairy Tales.txt        | 🏆 Score: 0.3815\n",
      "📄 Romeo and Juliet.txt           | 🏆 Score: 0.3751\n",
      "📄 Second Treatise of Government.txt | 🏆 Score: 0.2366\n",
      "📄 The Prince.txt                 | 🏆 Score: 0.0000\n",
      "--------------------------------------------------\n",
      "\n",
      "💡 QUERY: winter\n",
      "==================================================\n",
      "📄 Grimms' Fairy Tales.txt        | 🏆 Score: 0.3622\n",
      "📄 The Iliad.txt                  | 🏆 Score: 0.3122\n",
      "📄 Ulysses.txt                    | 🏆 Score: 0.3061\n",
      "📄 Middlemarch.txt                | 🏆 Score: 0.2766\n",
      "📄 The Count of Monte Cristo.txt  | 🏆 Score: 0.2640\n",
      "📄 Romeo and Juliet.txt           | 🏆 Score: 0.2566\n",
      "📄 The Prince.txt                 | 🏆 Score: 0.2408\n",
      "📄 Second Treatise of Government.txt | 🏆 Score: 0.2366\n",
      "📄 The Adventures of Tom Sawyer.txt | 🏆 Score: 0.0000\n",
      "--------------------------------------------------\n",
      "\n",
      "💡 QUERY: cloth\n",
      "==================================================\n",
      "📄 Grimms' Fairy Tales.txt        | 🏆 Score: 0.1251\n",
      "📄 Ulysses.txt                    | 🏆 Score: 0.1234\n",
      "📄 The Adventures of Tom Sawyer.txt | 🏆 Score: 0.1234\n",
      "📄 The Count of Monte Cristo.txt  | 🏆 Score: 0.1196\n",
      "📄 Middlemarch.txt                | 🏆 Score: 0.1172\n",
      "📄 Second Treatise of Government.txt | 🏆 Score: 0.1087\n",
      "📄 The Prince.txt                 | 🏆 Score: 0.1043\n",
      "📄 The Iliad.txt                  | 🏆 Score: 0.0883\n",
      "📄 Romeo and Juliet.txt           | 🏆 Score: 0.0810\n",
      "--------------------------------------------------\n",
      "\n",
      "💡 QUERY: live\n",
      "==================================================\n",
      "📄 Grimms' Fairy Tales.txt        | 🏆 Score: 0.1271\n",
      "📄 Middlemarch.txt                | 🏆 Score: 0.1269\n",
      "📄 The Count of Monte Cristo.txt  | 🏆 Score: 0.1267\n",
      "📄 Second Treatise of Government.txt | 🏆 Score: 0.1265\n",
      "📄 Romeo and Juliet.txt           | 🏆 Score: 0.1263\n",
      "📄 The Iliad.txt                  | 🏆 Score: 0.1263\n",
      "📄 Ulysses.txt                    | 🏆 Score: 0.1261\n",
      "📄 The Prince.txt                 | 🏆 Score: 0.1261\n",
      "📄 The Adventures of Tom Sawyer.txt | 🏆 Score: 0.1234\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from math import log\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Preprocessing and cleaning text\n",
    "def text_cleaner(text, stem='Stem'):\n",
    "    text = text.lower()  # Converting to lowercase\n",
    "    text = re.sub(r\"http\\S+\", '', text, flags=re.MULTILINE)  # Removing URLs\n",
    "    text = re.sub(r\"[^\\w\\s]\", '', text)  # Removing non-word, non-whitespace characters\n",
    "    text = re.sub(r\"[\\d]\", '', text)  # Removing numbers\n",
    "    cleaned_text = text.split()  # Tokenizing the text\n",
    "    \n",
    "    # Removing stop words\n",
    "    useless_words = stopwords.words(\"english\")\n",
    "    final_text = [word for word in cleaned_text if word not in useless_words]\n",
    "\n",
    "    # Applying stemming\n",
    "    if stem == 'Stem':\n",
    "        stemmer = PorterStemmer()\n",
    "        final_text = [stemmer.stem(word) for word in final_text]\n",
    "\n",
    "    return final_text\n",
    "\n",
    "# Loading the documents\n",
    "def load_documents(folder_path):\n",
    "    docs = {}\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith('.txt'):\n",
    "            with open(os.path.join(folder_path, filename), 'r') as file:\n",
    "                docs[filename] = text_cleaner(file.read())\n",
    "    return docs\n",
    "\n",
    "# Loading the queries\n",
    "def load_queries(query_file_path):\n",
    "    with open(query_file_path, 'r') as file:\n",
    "        return [text_cleaner(line.strip()) for line in file.readlines()]\n",
    "\n",
    "# Computing term and document frequencies\n",
    "def compute_statistics(docs):\n",
    "    doc_count = len(docs)\n",
    "    term_doc_freq = defaultdict(int)\n",
    "    term_freq = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "    for doc_id, words in docs.items():\n",
    "        word_set = set(words)\n",
    "        for word in words:\n",
    "            term_freq[doc_id][word] += 1\n",
    "        for word in word_set:\n",
    "            term_doc_freq[word] += 1\n",
    "\n",
    "    return term_freq, term_doc_freq, doc_count\n",
    "\n",
    "# Computing inverse document frequencies\n",
    "def compute_idf(term_doc_freq, doc_count):\n",
    "    idf = {}\n",
    "    for term, df in term_doc_freq.items():\n",
    "        idf[term] = log((doc_count - df + 0.5) / (df + 0.5) + 1)\n",
    "    return idf\n",
    "\n",
    "# Computing BM25 scores for documents given a query\n",
    "def compute_bm25(query, term_freq, idf, doc_count, avgdl, doc_lengths, k1=1.5, b=0.75):\n",
    "    scores = {}\n",
    "    for doc_id, doc_terms in term_freq.items():\n",
    "        score = 0\n",
    "        doc_length = doc_lengths[doc_id]\n",
    "        for term in query:\n",
    "            if term in doc_terms:\n",
    "                tf = doc_terms[term]\n",
    "                idf_term = idf.get(term, 0)\n",
    "                term_score = idf_term * ((tf * (k1 + 1)) / (tf + k1 * (1 - b + b * (doc_length / avgdl))))\n",
    "                score += term_score\n",
    "        scores[doc_id] = score\n",
    "    return scores\n",
    "\n",
    "# Retrieving and ranking documents using BM25 for given queries\n",
    "def retrieve_documents_bm25(folder_path, query_file_path):\n",
    "    docs = load_documents(folder_path)\n",
    "    queries = load_queries(query_file_path)\n",
    "\n",
    "    term_freq, term_doc_freq, doc_count = compute_statistics(docs)\n",
    "    doc_lengths = {doc_id: len(words) for doc_id, words in docs.items()}\n",
    "    avgdl = np.mean(list(doc_lengths.values()))\n",
    "    idf = compute_idf(term_doc_freq, doc_count)\n",
    "\n",
    "    for query in queries:\n",
    "        scores = compute_bm25(query, term_freq, idf, doc_count, avgdl, doc_lengths)\n",
    "        ranked_docs = sorted(scores.items(), key=lambda item: item[1], reverse=True)\n",
    "        \n",
    "        # Unique formatted output\n",
    "        print(f\"💡 QUERY: {' '.join(query)}\")\n",
    "        print(\"=\"*50)\n",
    "        for doc_id, score in ranked_docs:\n",
    "            print(f\"📄 {doc_id:<30} | 🏆 Score: {score:.4f}\")\n",
    "        print(\"-\"*50 + \"\\n\")\n",
    "\n",
    "# Function to assign random relevance scores\n",
    "def assign_random_relevance(queries, documents, relevance_scale=(0, 1)):\n",
    "    relevance_scores = {}\n",
    "    for query in queries:\n",
    "        relevance_scores[query] = {}\n",
    "        for doc in documents:\n",
    "            relevance_scores[query][doc] = np.random.randint(relevance_scale[0], relevance_scale[1])\n",
    "    return relevance_scores\n",
    "\n",
    "# Function to save relevance scores to a file\n",
    "def save_relevance_scores_to_file(relevance_scores, output_file):\n",
    "    with open(output_file, 'w') as f:\n",
    "        for query, doc_scores in relevance_scores.items():\n",
    "            for doc, score in doc_scores.items():\n",
    "                f.write(f\"{query},{doc},{score}\\n\")\n",
    "\n",
    "# Example usage:\n",
    "folder_path = '../../Final Project/Dataset'\n",
    "query_file_path = '../../Final Project/queries.txt'\n",
    "retrieve_documents_bm25(folder_path, query_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
