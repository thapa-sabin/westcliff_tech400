{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af801891-54c7-49b8-bd14-6438c1dc3531",
   "metadata": {},
   "source": [
    "# Checking the similarity between documents using cosine similarity in a vector space model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa749a8-a4f0-46ed-b240-e473fdb2dd28",
   "metadata": {},
   "source": [
    "## Importing required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74947ece-6d49-4299-ac44-6c6ce294db04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import math\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40135b3a-57af-4768-9619-fd690eebd0ad",
   "metadata": {},
   "source": [
    "## Cleaning and Tokenizing the text from documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e81102e6-091d-4597-8ee0-117828023eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the text cleaning function\n",
    "def text_cleaner(text, stem='Stem'):\n",
    "    # Converting text to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Removing URLs\n",
    "    text = re.sub(r\"http\\S+\", '', text, flags=re.MULTILINE)\n",
    "\n",
    "    # Removing non-word and non-whitespace characters\n",
    "    text = re.sub(r\"[^\\w\\s]\", '', text)\n",
    "\n",
    "    # Removing numbers\n",
    "    text = re.sub(r\"[\\d]\", '', text)\n",
    "\n",
    "    # Tokenizing text\n",
    "    tokens = text.split()\n",
    "\n",
    "    # Removing stop words\n",
    "    stop_words = stopwords.words(\"english\")\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "    # Stemming the words if requested\n",
    "    if stem == 'Stem':\n",
    "        stemmer = PorterStemmer()\n",
    "        tokens = [stemmer.stem(word) for word in tokens]\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd94473e-614c-4414-984b-413d2385bb7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate term frequency (TF)\n",
    "def term_frequency(term, document):\n",
    "    return document.count(term) / len(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0b41b3f-24be-4271-b26e-a4575943aa6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate inverse document frequency (IDF)\n",
    "def inverse_document_frequency(term, all_documents):\n",
    "    num_docs_containing_term = sum(1 for doc in all_documents if term in doc)\n",
    "    return math.log(len(all_documents) / (1 + num_docs_containing_term))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5be58227-bd32-4848-9214-466392a968dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute TF-IDF for a document\n",
    "def compute_tfidf(document, all_documents, vocab):\n",
    "    tfidf_vector = []\n",
    "    for term in vocab:\n",
    "        tf = term_frequency(term, document)\n",
    "        idf = inverse_document_frequency(term, all_documents)\n",
    "        tfidf_vector.append(tf * idf)\n",
    "    return np.array(tfidf_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "18610c95-f692-4187-b70a-3dd3e6f13af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute cosine similarity between two vectors\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    # Compute norms and handle zero vector case\n",
    "    norm_vec1 = np.linalg.norm(vec1)\n",
    "    norm_vec2 = np.linalg.norm(vec2)\n",
    "    if norm_vec1 == 0 or norm_vec2 == 0:\n",
    "        return 0.0  # If either vector is all zeros, similarity is 0\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    return dot_product / (norm_vec1 * norm_vec2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a70784b-28e9-492c-9c35-6b759a7a8d4f",
   "metadata": {},
   "source": [
    "## Main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ea8c34c-341e-474d-89da-8f22ab27cfc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cosine similarities for query 'teacher' (ranked):\n",
      "Document The Teacher’s Legacy.txt: 0.2547\n",
      "Document The Writer’s Creative Struggle.txt: 0.0000\n",
      "Document The Surgeon’s Miracle.txt: 0.0000\n",
      "Document The Scientist’s Groundbreaking Discovery.txt: 0.0000\n",
      "Document The Engineer's Invention.txt: 0.0000\n",
      "Document The Discovery of a Hidden Cave.txt: 0.0000\n",
      "Document The Astronaut’s Journey to Mars.txt: 0.0000\n",
      "Document The Artist’s Masterpiece.txt: 0.0000\n",
      "Document Adventure on the High Seas.txt: 0.0000\n",
      "Document A Day in the Life of a Chef.txt: 0.0000\n",
      "\n",
      "Cosine similarities for query 'Dr.' (ranked):\n",
      "Document The Surgeon’s Miracle.txt: 0.2616\n",
      "Document The Scientist’s Groundbreaking Discovery.txt: 0.2605\n",
      "Document The Writer’s Creative Struggle.txt: 0.0000\n",
      "Document The Teacher’s Legacy.txt: 0.0000\n",
      "Document The Engineer's Invention.txt: 0.0000\n",
      "Document The Discovery of a Hidden Cave.txt: 0.0000\n",
      "Document The Astronaut’s Journey to Mars.txt: 0.0000\n",
      "Document The Artist’s Masterpiece.txt: 0.0000\n",
      "Document Adventure on the High Seas.txt: 0.0000\n",
      "Document A Day in the Life of a Chef.txt: 0.0000\n",
      "\n",
      "Cosine similarities for query 'teacher' (ranked):\n",
      "Document The Teacher’s Legacy.txt: 0.2547\n",
      "Document The Writer’s Creative Struggle.txt: 0.0000\n",
      "Document The Surgeon’s Miracle.txt: 0.0000\n",
      "Document The Scientist’s Groundbreaking Discovery.txt: 0.0000\n",
      "Document The Engineer's Invention.txt: 0.0000\n",
      "Document The Discovery of a Hidden Cave.txt: 0.0000\n",
      "Document The Astronaut’s Journey to Mars.txt: 0.0000\n",
      "Document The Artist’s Masterpiece.txt: 0.0000\n",
      "Document Adventure on the High Seas.txt: 0.0000\n",
      "Document A Day in the Life of a Chef.txt: 0.0000\n",
      "\n",
      "Cosine similarities for query 'live' (ranked):\n",
      "Document The Teacher’s Legacy.txt: 0.1097\n",
      "Document The Astronaut’s Journey to Mars.txt: 0.0419\n",
      "Document Adventure on the High Seas.txt: 0.0377\n",
      "Document The Surgeon’s Miracle.txt: 0.0376\n",
      "Document The Writer’s Creative Struggle.txt: 0.0000\n",
      "Document The Scientist’s Groundbreaking Discovery.txt: 0.0000\n",
      "Document The Engineer's Invention.txt: 0.0000\n",
      "Document The Discovery of a Hidden Cave.txt: 0.0000\n",
      "Document The Artist’s Masterpiece.txt: 0.0000\n",
      "Document A Day in the Life of a Chef.txt: 0.0000\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    directory = './dataset/'\n",
    "    \n",
    "    # Reading all files from the directory\n",
    "    docs = []\n",
    "    filenames = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            with open(os.path.join(directory, filename), \"r\") as file:\n",
    "                content = file.read()\n",
    "                docs.append(content)\n",
    "                filenames.append(filename)\n",
    "                \n",
    "    queries = [\"teacher\", \"Dr.\", \"teacher\", \"live\"]\n",
    "    \n",
    "    # Tokenizing both the documents and queries\n",
    "    tokenized_docs = [text_cleaner(doc) for doc in docs]\n",
    "    tokenized_queries = [text_cleaner(query) for query in queries]\n",
    "    \n",
    "    # Building the vocabulary\n",
    "    vocab = sorted(set([word for doc in tokenized_docs for word in doc]))\n",
    "    \n",
    "    # Calculate TF-IDF vectors for documents and queries\n",
    "    doc_tfidf_vectors = [compute_tfidf(doc, tokenized_docs, vocab) for doc in tokenized_docs]\n",
    "    query_tfidf_vectors = [compute_tfidf(query, tokenized_docs, vocab) for query in tokenized_queries]\n",
    "\n",
    "    # Calculate cosine similarities\n",
    "    cosine_similarities = []\n",
    "    for query_vector in query_tfidf_vectors:\n",
    "        similarities = [cosine_similarity(query_vector, doc_vector) for doc_vector in doc_tfidf_vectors]\n",
    "        cosine_similarities.append(similarities)\n",
    "    \n",
    "    # Writing the ranked results to a text file\n",
    "    with open(\"results_sabinthapa.txt\", \"w\") as output_file:\n",
    "        for i, query in enumerate(queries):\n",
    "            output_file.write(f\"\\nCosine similarities for query '{query}':\\n\")\n",
    "            \n",
    "            # Create a list of document similarities with document names\n",
    "            ranked_docs = sorted(zip(cosine_similarities[i], filenames), reverse=True)\n",
    "            \n",
    "            for similarity, filename in ranked_docs:\n",
    "                output_file.write(f\"Document {filename}: {similarity:.4f}\\n\")\n",
    "\n",
    "    # Printing the ranking\n",
    "    for i, query in enumerate(queries):\n",
    "        print(f\"\\nCosine similarities for query '{query}' (ranked):\")\n",
    "        \n",
    "        ranked_docs = sorted(zip(cosine_similarities[i], filenames), reverse=True)\n",
    "        \n",
    "        for similarity, filename in ranked_docs:\n",
    "            print(f\"Document {filename}: {similarity:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
