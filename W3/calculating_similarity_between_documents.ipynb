{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af801891-54c7-49b8-bd14-6438c1dc3531",
   "metadata": {},
   "source": [
    "# Checking the similarity between documents using cosine similarity in a vector space model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa749a8-a4f0-46ed-b240-e473fdb2dd28",
   "metadata": {},
   "source": [
    "## Importing required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74947ece-6d49-4299-ac44-6c6ce294db04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40135b3a-57af-4768-9619-fd690eebd0ad",
   "metadata": {},
   "source": [
    "## Cleaning and Tokenizing the text from documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e81102e6-091d-4597-8ee0-117828023eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the text cleaning function\n",
    "def text_cleaner(text, stem='Stem'):\n",
    "    # Converting text to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Removing URLs\n",
    "    text = re.sub(r\"http\\S+\", '', text, flags=re.MULTILINE)\n",
    "\n",
    "    # Removing non-word and non-whitespace characters\n",
    "    text = re.sub(r\"[^\\w\\s]\", '', text)\n",
    "\n",
    "    # Removing numbers\n",
    "    text = re.sub(r\"[\\d]\", '', text)\n",
    "\n",
    "    # Tokenizing text\n",
    "    tokens = text.split()\n",
    "\n",
    "    # Removing stop words\n",
    "    stop_words = stopwords.words(\"english\")\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "    # Stemming the words if requested\n",
    "    if stem == 'Stem':\n",
    "        stemmer = PorterStemmer()\n",
    "        tokens = [stemmer.stem(word) for word in tokens]\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4d0553-99e8-48cc-80ad-0a9ec94b1ffb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fd94473e-614c-4414-984b-413d2385bb7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate term frequency (TF)\n",
    "def term_frequency(term, document):\n",
    "    return document.count(term) / len(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f0b41b3f-24be-4271-b26e-a4575943aa6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate inverse document frequency (IDF)\n",
    "def inverse_document_frequency(term, all_documents):\n",
    "    num_docs_containing_term = sum(1 for doc in all_documents if term in doc)\n",
    "    return math.log(len(all_documents) / (1 + num_docs_containing_term))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5be58227-bd32-4848-9214-466392a968dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute TF-IDF for a document\n",
    "def compute_tfidf(document, all_documents, vocab):\n",
    "    tfidf_vector = []\n",
    "    for term in vocab:\n",
    "        tf = term_frequency(term, document)\n",
    "        idf = inverse_document_frequency(term, all_documents)\n",
    "        tfidf_vector.append(tf * idf)\n",
    "    return np.array(tfidf_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "18610c95-f692-4187-b70a-3dd3e6f13af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute cosine similarity between two vectors\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    # Compute norms and handle zero vector case\n",
    "    norm_vec1 = np.linalg.norm(vec1)\n",
    "    norm_vec2 = np.linalg.norm(vec2)\n",
    "    if norm_vec1 == 0 or norm_vec2 == 0:\n",
    "        return 0.0  # If either vector is all zeros, similarity is 0\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    return dot_product / (norm_vec1 * norm_vec2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a70784b-28e9-492c-9c35-6b759a7a8d4f",
   "metadata": {},
   "source": [
    "## Main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8ea8c34c-341e-474d-89da-8f22ab27cfc1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenize' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 54\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDocument \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msimilarity\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 54\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[11], line 17\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m queries \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mteacher\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDr.\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mteacher live\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Tokenizing both the documents and queries\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m tokenized_docs \u001b[38;5;241m=\u001b[39m [tokenize(doc) \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m docs]\n\u001b[1;32m     18\u001b[0m tokenized_queries \u001b[38;5;241m=\u001b[39m [tokenize(query) \u001b[38;5;28;01mfor\u001b[39;00m query \u001b[38;5;129;01min\u001b[39;00m queries]\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Building the vocabulary\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[11], line 17\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     14\u001b[0m queries \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mteacher\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDr.\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mteacher live\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Tokenizing both the documents and queries\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m tokenized_docs \u001b[38;5;241m=\u001b[39m [\u001b[43mtokenize\u001b[49m(doc) \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m docs]\n\u001b[1;32m     18\u001b[0m tokenized_queries \u001b[38;5;241m=\u001b[39m [tokenize(query) \u001b[38;5;28;01mfor\u001b[39;00m query \u001b[38;5;129;01min\u001b[39;00m queries]\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Building the vocabulary\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenize' is not defined"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    directory = './dataset/'\n",
    "    \n",
    "    # Reading all files from the directory\n",
    "    docs = []\n",
    "    filenames = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            with open(os.path.join(directory, filename), \"r\") as file:\n",
    "                content = file.read()\n",
    "                docs.append(content)\n",
    "                filenames.append(filename)\n",
    "                \n",
    "    queries = [\"teacher\", \"Dr.\", \"teacher live\"]\n",
    "    \n",
    "    # Tokenizing both the documents and queries\n",
    "    tokenized_docs = [tokenize(doc) for doc in docs]\n",
    "    tokenized_queries = [tokenize(query) for query in queries]\n",
    "    \n",
    "    # Building the vocabulary\n",
    "    vocab = sorted(set([word for doc in tokenized_docs for word in doc]))\n",
    "    \n",
    "    # Calculate TF-IDF vectors for documents and queries\n",
    "    doc_tfidf_vectors = [compute_tfidf(doc, tokenized_docs, vocab) for doc in tokenized_docs]\n",
    "    query_tfidf_vectors = [compute_tfidf(query, tokenized_docs, vocab) for query in tokenized_queries]\n",
    "\n",
    "    # Calculate cosine similarities\n",
    "    cosine_similarities = []\n",
    "    for query_vector in query_tfidf_vectors:\n",
    "        similarities = [cosine_similarity(query_vector, doc_vector) for doc_vector in doc_tfidf_vectors]\n",
    "        cosine_similarities.append(similarities)\n",
    "    \n",
    "    # Writing the ranked results to a text file\n",
    "    with open(\"results_sabinthapa.txt\", \"w\") as output_file:\n",
    "        for i, query in enumerate(queries):\n",
    "            output_file.write(f\"\\nCosine similarities for query '{query}':\\n\")\n",
    "            \n",
    "            # Create a list of document similarities with document names\n",
    "            ranked_docs = sorted(zip(cosine_similarities[i], filenames), reverse=True)\n",
    "            \n",
    "            for similarity, filename in ranked_docs:\n",
    "                output_file.write(f\"Document {filename}: {similarity:.4f}\\n\")\n",
    "\n",
    "    # Printing the ranking\n",
    "    for i, query in enumerate(queries):\n",
    "        print(f\"\\nCosine similarities for query '{query}' (ranked):\")\n",
    "        \n",
    "        ranked_docs = sorted(zip(cosine_similarities[i], filenames), reverse=True)\n",
    "        \n",
    "        for similarity, filename in ranked_docs:\n",
    "            print(f\"Document {filename}: {similarity:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26191480-954a-4192-adc3-bf743f808ad9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
