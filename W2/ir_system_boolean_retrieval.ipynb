{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3b4f7f0-1d5c-462b-9f5a-1c1fa1f31b20",
   "metadata": {},
   "source": [
    "# IR System using Boolean Retrieval Technique"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80d85e2-3882-4da0-b9e1-b57c6c5c7dde",
   "metadata": {},
   "source": [
    "## Function that clears the Boiletplate text from the beginning of the book, and the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e62276f-d1e4-44ad-8331-7bc625a7ec8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import nltk\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2dad4bb-028e-4fb3-bb71-b7f791cefe01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the function to strip Project Gutenberg boilerplate\n",
    "def strip_boilerplate(text):\n",
    "    # Defining the start and end markers\n",
    "    start_marker = r'\\*\\*\\* START OF THE PROJECT GUTENBERG EBOOK .* \\*\\*\\*'\n",
    "    end_marker = r'\\*\\*\\* END OF THE PROJECT GUTENBERG EBOOK .* \\*\\*\\*'\n",
    "    \n",
    "    # Using regex to extract text between the markers\n",
    "    match = re.search(f'{start_marker}(.*?){end_marker}', text, re.DOTALL)\n",
    "    \n",
    "    if match:\n",
    "        # Returning the text between the markers\n",
    "        return match.group(1).strip()\n",
    "    else:\n",
    "        return \"Boilerplate markers not found.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9198a667-21c6-4e7f-9a76-cb281c9bb488",
   "metadata": {},
   "source": [
    "## Text pre-processing (Documents-only), and trying the system out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "814c7368-5208-41d6-a493-0a12ed6a5a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the text cleaning function\n",
    "def text_cleaner(text, stem='Stem'):\n",
    "    # Converting text to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Removing URLs\n",
    "    text = re.sub(r\"http\\S+\", '', text, flags=re.MULTILINE)\n",
    "\n",
    "    # Removing non-word and non-whitespace characters\n",
    "    text = re.sub(r\"[^\\w\\s]\", '', text)\n",
    "\n",
    "    # Removing numbers\n",
    "    text = re.sub(r\"[\\d]\", '', text)\n",
    "\n",
    "    # Tokenizing text\n",
    "    tokens = text.split()\n",
    "\n",
    "    # Removing stop words\n",
    "    stop_words = stopwords.words(\"english\")\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "    # Stemming the words if requested\n",
    "    if stem == 'Stem':\n",
    "        stemmer = PorterStemmer()\n",
    "        tokens = [stemmer.stem(word) for word in tokens]\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8af1f998-6db8-4868-8502-94fdfe6bc686",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the function to read and clean all text documents in a directory\n",
    "def clean_and_add_documents(directory, inverted_index, doc_id_to_filename, dictionary):\n",
    "    doc_id_counter = 0\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith('.txt'):\n",
    "            # Reading content of each file\n",
    "            with open(os.path.join(directory, filename), 'r', encoding='utf-8') as file:\n",
    "                content = file.read()\n",
    "\n",
    "                # Stripping the boilerplate if present\n",
    "                content = strip_boilerplate(content)\n",
    "\n",
    "                # Cleaning the content\n",
    "                cleaned_content = text_cleaner(content)\n",
    "\n",
    "                # Adding cleaned document to the inverted index\n",
    "                add_document_to_index(cleaned_content, filename, doc_id_counter, inverted_index, doc_id_to_filename, dictionary)\n",
    "                doc_id_counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "851795c2-c939-4351-94f2-3fd21350fab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the function to add a cleaned document to the inverted index\n",
    "def add_document_to_index(content, filename, doc_id, inverted_index, doc_id_to_filename, dictionary):\n",
    "    # Storing document filename with doc_id\n",
    "    doc_id_to_filename[doc_id] = filename\n",
    "\n",
    "    # Looping through each word in the document\n",
    "    for word in content:\n",
    "        if word not in dictionary:\n",
    "            dictionary[word] = len(dictionary)  # Assigning unique ID to each word\n",
    "        word_id = dictionary[word]\n",
    "        # Adding doc_id to the set for this word\n",
    "        inverted_index[word_id].add(doc_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a7da940-6ca4-45fe-97cc-99efd522944d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the Boolean retrieval function to process a query\n",
    "def boolean_retrieval(query, dictionary, inverted_index):\n",
    "    # Cleaning the query and splitting into words\n",
    "    words = text_cleaner(query)\n",
    "\n",
    "    result_set = None\n",
    "    # Retrieving documents for each word in the query\n",
    "    for word in words:\n",
    "        word_id = dictionary.get(word, -1)\n",
    "        if word_id == -1:\n",
    "            return set()  # Word not found, returning empty set\n",
    "        word_set = inverted_index[word_id]\n",
    "        if result_set is None:\n",
    "            result_set = word_set  # Initializing result set\n",
    "        else:\n",
    "            result_set &= word_set  # Performing intersection of result sets for each word\n",
    "\n",
    "    return result_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d4848e41-9cb3-40aa-a6b5-be1ded0c3f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the function to get filenames of the documents matching the query\n",
    "def get_filenames(doc_ids, doc_id_to_filename):\n",
    "    return [doc_id_to_filename[doc_id] for doc_id in doc_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4805a231-0142-4100-bd6b-8b2a200b95f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document IDs for query 'sailor': {0, 2}\n",
      "Document names for query 'sailor': ['pg1184.txt', 'pg6130.txt']\n",
      "Document IDs for query 'Machiavelli Florentines faint': {0}\n",
      "Document names for query 'Machiavelli Florentines faint': ['pg1184.txt']\n"
     ]
    }
   ],
   "source": [
    "# Defining the main function to process the documents and run queries\n",
    "def main():\n",
    "    # Initializing necessary structures\n",
    "    inverted_index = defaultdict(set)\n",
    "    doc_id_to_filename = {}\n",
    "    dictionary = {}\n",
    "\n",
    "    # Specifying the directory with text documents\n",
    "    directory = './dataset/'\n",
    "\n",
    "    # Cleaning and indexing all documents in the directory\n",
    "    clean_and_add_documents(directory, inverted_index, doc_id_to_filename, dictionary)\n",
    "\n",
    "    # Defining queries\n",
    "    query1 = \"sailor\"\n",
    "    query2 = \"Machiavelli Florentines faint\"\n",
    "\n",
    "    # Performing Boolean retrieval for both queries\n",
    "    result1 = boolean_retrieval(query1, dictionary, inverted_index)\n",
    "    result2 = boolean_retrieval(query2, dictionary, inverted_index)\n",
    "\n",
    "    # Printing results for query1\n",
    "    print(f\"Document IDs for query '{query1}':\", result1)\n",
    "    print(f\"Document names for query '{query1}':\", get_filenames(result1, doc_id_to_filename))\n",
    "\n",
    "    # Printing results for query2\n",
    "    print(f\"Document IDs for query '{query2}':\", result2)\n",
    "    print(f\"Document names for query '{query2}':\", get_filenames(result2, doc_id_to_filename))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3064b85-4eab-464e-b1ab-2804521c3571",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
