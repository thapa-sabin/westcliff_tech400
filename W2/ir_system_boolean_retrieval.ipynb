{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3b4f7f0-1d5c-462b-9f5a-1c1fa1f31b20",
   "metadata": {},
   "source": [
    "# IR System using Boolean Retrieval Technique"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80d85e2-3882-4da0-b9e1-b57c6c5c7dde",
   "metadata": {},
   "source": [
    "## Function that clears the Boiletplate text from the beginning of the book, and the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d2dad4bb-028e-4fb3-bb71-b7f791cefe01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_boilerplate(text):\n",
    "    # Define the start and end markers\n",
    "    start_marker = r'\\*\\*\\* START OF THE PROJECT GUTENBERG EBOOK .* \\*\\*\\*'\n",
    "    end_marker = r'\\*\\*\\* END OF THE PROJECT GUTENBERG EBOOK .* \\*\\*\\*'\n",
    "    \n",
    "    # Use regex to extract text between the markers\n",
    "    match = re.search(f'{start_marker}(.*?){end_marker}', text, re.DOTALL)\n",
    "    \n",
    "    if match:\n",
    "        # Return the text between the markers\n",
    "        return match.group(1).strip()\n",
    "    else:\n",
    "        return \"Boilerplate markers not found.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9198a667-21c6-4e7f-9a76-cb281c9bb488",
   "metadata": {},
   "source": [
    "## Text pre-processing (Documents-only), and trying the system out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814c7368-5208-41d6-a493-0a12ed6a5a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from collections import defaultdict\n",
    "\n",
    "# Text cleaning function\n",
    "def text_cleaner(text, stem='Stem'):\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove URLs\n",
    "    text = re.sub(r\"http\\S+\", '', text, flags=re.MULTILINE)\n",
    "\n",
    "    # Remove non-word and non-whitespace characters\n",
    "    text = re.sub(r\"[^\\w\\s]\", '', text)\n",
    "\n",
    "    # Remove numbers\n",
    "    text = re.sub(r\"[\\d]\", '', text)\n",
    "\n",
    "    # Tokenize text\n",
    "    tokens = text.split()\n",
    "\n",
    "    # Remove stop words\n",
    "    stop_words = stopwords.words(\"english\")\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "    # Stem the words if requested\n",
    "    if stem == 'Stem':\n",
    "        stemmer = PorterStemmer()\n",
    "        tokens = [stemmer.stem(word) for word in tokens]\n",
    "\n",
    "    return tokens\n",
    "\n",
    "# Function to read and clean all text documents in a directory\n",
    "def clean_and_add_documents(directory, inverted_index, doc_id_to_filename, dictionary):\n",
    "    doc_id_counter = 0\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith('.txt'):\n",
    "            # Read content of each file\n",
    "            with open(os.path.join(directory, filename), 'r', encoding='utf-8') as file:\n",
    "                content = file.read()\n",
    "\n",
    "                # Cleaning the content\n",
    "                cleaned_content = text_cleaner(content)\n",
    "\n",
    "                # Adding cleaned document to the inverted index\n",
    "                add_document_to_index(cleaned_content, filename, doc_id_counter, inverted_index, doc_id_to_filename, dictionary)\n",
    "                doc_id_counter += 1\n",
    "\n",
    "# Function to add a cleaned document to the inverted index\n",
    "def add_document_to_index(content, filename, doc_id, inverted_index, doc_id_to_filename, dictionary):\n",
    "    # Store document filename with doc_id\n",
    "    doc_id_to_filename[doc_id] = filename\n",
    "\n",
    "    # For each word in the document\n",
    "    for word in content:\n",
    "        if word not in dictionary:\n",
    "            dictionary[word] = len(dictionary)  # Assigning unique ID to each word\n",
    "        word_id = dictionary[word]\n",
    "        inverted_index[word_id].add(doc_id)  # Adding doc_id to the set for this word\n",
    "\n",
    "# Boolean retrieval function to process a query\n",
    "def boolean_retrieval(query, dictionary, inverted_index):\n",
    "    # Clean the query and split into words\n",
    "    words = text_cleaner(query)\n",
    "\n",
    "    result_set = None\n",
    "    # Retrieve documents for each word in the query\n",
    "    for word in words:\n",
    "        word_id = dictionary.get(word, -1)\n",
    "        if word_id == -1:\n",
    "            return set()  # Word not found, return empty set\n",
    "        word_set = inverted_index[word_id]\n",
    "        if result_set is None:\n",
    "            result_set = word_set  # Initialize result set\n",
    "        else:\n",
    "            result_set &= word_set  # Intersection of result sets for each word\n",
    "\n",
    "    return result_set\n",
    "\n",
    "# Function to get filenames of the documents matching the query\n",
    "def get_filenames(doc_ids, doc_id_to_filename):\n",
    "    return [doc_id_to_filename[doc_id] for doc_id in doc_ids]\n",
    "\n",
    "# Main function to process the documents and run queries\n",
    "def main():\n",
    "    # Initializing necessary structures\n",
    "    inverted_index = defaultdict(set)\n",
    "    doc_id_to_filename = {}\n",
    "    dictionary = {}\n",
    "\n",
    "    # Directory with text documents\n",
    "    directory = './dataset/'\n",
    "\n",
    "    # Clean and index all documents in the directory\n",
    "    clean_and_add_documents(directory, inverted_index, doc_id_to_filename, dictionary)\n",
    "\n",
    "    # Queries\n",
    "    query1 = \"sailor\"\n",
    "    query2 = \"Machiavelli Florentines faint\"\n",
    "\n",
    "    # Perform Boolean retrieval for both queries\n",
    "    result1 = boolean_retrieval(query1, dictionary, inverted_index)\n",
    "    result2 = boolean_retrieval(query2, dictionary, inverted_index)\n",
    "\n",
    "    # Print results for query1\n",
    "    print(f\"Document IDs for query '{query1}':\", result1)\n",
    "    print(f\"Document names for query '{query1}':\", get_filenames(result1, doc_id_to_filename))\n",
    "\n",
    "    # Print results for query2\n",
    "    print(f\"Document IDs for query '{query2}':\", result2)\n",
    "    print(f\"Document names for query '{query2}':\", get_filenames(result2, doc_id_to_filename))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
